<!DOCTYPE html><html>

<head>
<meta charset="utf-8">
<title>all</title>
<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>
</head>
<body>


<h1 id="toc_0">RHEL HA Workshop</h1>

<p><strong>Author</strong>: Pat Harrison &lt;<a href="mailto:pharriso@redhat.com">pharriso@redhat.com</a>&gt;</p>
<p><strong>Thanks</strong>: Thanks to Rhys Oxenham. I stole his html template.</p>

<h2 id="toc_2">Workshop Agenda</h2>

<h3 id="toc_3">Install and Configure</h3>

<table>
<thead>
<tr>
<th>Item</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Create Cluster</td>
<td>Create a three node cluster</td>
</tr>
<tr>
<td>Fencing</td>
<td>Configure and test fencing</td>
</tr>
<tr>
<td>Prepare resources</td>
<td>Prepare the resources which will be managed by the cluster</td>
</tr>
<tr>
<td>Cluster Resources</td>
<td>Create and manage cluster resources and nodes</td>
</tr>
<tr>
<td>Manage cluster resource behaviour</td>
<td>Manage node preference and resource stickiness</td>
</tr>
<tr>
<td>Web UI</td>
<td>Access pcsd Web UI</td>
</tr>
<tr>
<td>Adding Nodes</td>
<td>Adding a fourth node to the cluster</td>
</tr>
<tr>
<td>Manage LVM </td>
<td>Activating LVM outside of the cluster</td>
</tr>
</tbody>
</table>

<h3 id="toc_3">Testing</h3>

<table>
<thead>
<tr>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Offline and Online resources</td>
</tr>
<tr>
<td>Controlled failover testing</td>
</tr>
<tr>
<td>Shutdown and Restart of cluster</td>
</tr>
<tr>
<td>Stop resource outside of cluster</td>
</tr>
<tr>
<td>Resource group failure testing</td>
</tr>
<tr>
<td>Crash cluster node</td>
</tr>
</tbody>
</table>

<h3 id="toc_3">GFS clustered filesystem</h3>

<table>
<thead>
<tr>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Reset labs</td>
<td>Re-deploy virtual environment</td>
</tr>
<tr>
<td>Create cluster</td>
<td>Create a new 3 node cluster</td>
</tr>
<tr>
<td>Configure GFS2</td>
<td>Configure GFS2 and related cluster resources</td>
</tr>
</tbody>
</table>

<h2 id="toc_7">Labs Overview</h2>

<p><center>
<img src="images/RHEL_HA_Networks.png"/>
</center></p>

<h2 id="toc_8">Principles and Conventions of the Labs</h2>

<h3 id="toc_9">Code Entry</h3>

<p>Many of the labs will expect you to type commands into the CLI over a secure shell connection to a set of hosts provided. Each command entry will be highlighted, and where necessary the node we&#39;re expecting you to use will be identified. However, for clarity the conventions are identified below.</p>

<p>This is a command entry box:</p>

<pre><code class="language-none"># uname -a
Linux s01.fab.redhat.com 2.6.32-504.16.2.el6.x86_64 #1 SMP Tue Mar 10 17:01:00 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux</code></pre>

<p>Any commands that we expect you to run on a particular <strong>node</strong> will start with the node name and then a hash (<strong>nodea #</strong>), for example:</p>

<pre><code class="language-none">nodea # whoami
root</code></pre>


<h1 id="toc_11">RHEL HA Overview</h1>

<h2 id="toc_13">Corosync</h2>

<p>Corosync is the framework used by Pacemaker for handling communication between the cluster nodes. Corosync is also Pacemaker’s source of membership and quorum data.</p>

<h2 id="toc_13">Pacemaker</h2>

<p>This is the component responsible for all cluster-related activities, such as monitoring cluster membership, managing the services and resources, and fencing cluster members.</p>

<h2 id="toc_13">Fencing</h2>

<p> Fencing is the disconnection of a node from the cluster's shared storage. Fencing cuts off I/O from shared storage, thus ensuring data integrity. The cluster infrastructure performs fencing through the STONITH facility.
When Pacemaker determines that a node has failed, it communicates to other cluster-infrastructure components that the node has failed. STONITH fences the failed node when notified of the failure.</p>

<h2 id="toc_13">Quorum</h2>

<p>In order to maintain cluster integrity and availability, cluster systems use a concept known as quorum to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.</p>

<h1 id="toc_17">Install and Configure</h1>

<h2 id="toc_18">Create a basic cluster</h2>

<p>We are going to create a 3 node cluster comprising of <strong>nodea</strong>, <strong>nodeb</strong> and <strong>nodec</strong>. Start by installing the RHEL HA packages on <strong>each node</strong>.</p>

<pre><code class="language-none"># yum install pcs pacemaker fence-agents-all</code></pre>

<p>Enable cluster communications through the firewall on <strong>each node</strong>.</p>

<pre><code class="language-none"># firewall-cmd --permanent --add-service=high-availability
# firewall-cmd --add-service=high-availability
</code></pre>

<p>Set password for the pcs administration account on <strong>all nodes</strong>.</p>

<pre><code class="language-none"># echo Redhat123 | passwd --stdin hacluster</code></pre>

<p>Start and enable the pcs daemon on <strong>each node</strong>.</p>

<pre><code class="language-none"># systemctl enable pcsd && systemctl start pcsd </code></pre>

<p>On <strong>nodea</strong>, authenticate the pcs admin user against each node in the cluster.</p>

<pre><code class="language-none">nodea # pcs cluster auth nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com</code></pre>

<p>On <strong>nodea</strong>, create and start a cluster called cluster1 consisting of our 3 nodes.</p>

<pre><code class="language-none">nodea # pcs cluster setup --start --enable --name cluster1 nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com</code></pre>

<p>Verify our cluster has been created succesfully.</p>

<pre><code class="language-none">nodea # pcs status
Cluster name: cluster1
WARNING: no stonith devices and stonith-enabled is not false
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 10:31:22 2018
Last change: Thu May 24 10:29:00 2018 by hacluster via crmd on nodea.example.com

3 nodes configured
0 resources configured

Online: [ nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com ]

No resources


Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled</code></pre>

<p>Note the warning in the output telling us we haven't enabled fencing. We'll fix that next.</p>

<h2 id="toc_18">Configure Fencing</h2>

<p>We can list all of the available fencing agents.</p>

<pre><code class="language-none">nodea # pcs stonith list</code></pre>

<p>Let's look at all of the available options for the IPMI fencing agent.</p>

<pre><code class="language-none">nodea # pcs stonith describe fence_ipmilan
fence_ipmilan - Fence agent for IPMI

fence_ipmilan is an I/O Fencing agentwhich can be used with machines controlled by IPMI.This agent calls support software ipmitool (http://ipmitool.sf.net/). WARNING! This fence agent might report success before the node is powered off. You should use -m/method onoff if your fence device works correctly with that option.

Stonith options:
  ipport: TCP/UDP port to use for connection with device
  hexadecimal_kg: Hexadecimal-encoded Kg key for IPMIv2 authentication
  port: IP address or hostname of fencing device (together with --port-as-ip)
  inet6_only: Forces agent to use IPv6 addresses only
  ipaddr: IP Address or Hostname
  passwd_script: Script to retrieve password
  method: Method to fence (onoff|cycle)
  inet4_only: Forces agent to use IPv4 addresses only
  passwd: Login password or passphrase
  lanplus: Use Lanplus to improve security of connection
  auth: IPMI Lan Auth type.
  cipher: Ciphersuite to use (same as ipmitool -C parameter)
  target: Bridge IPMI requests to the remote target address
  privlvl: Privilege level on IPMI device
  timeout: Timeout (sec) for IPMI operation
  login: Login Name
  verbose: Verbose mode
  debug: Write debug information to given file
  power_wait: Wait X seconds after issuing ON/OFF
  login_timeout: Wait X seconds for cmd prompt after login
  delay: Wait X seconds before fencing is started
  power_timeout: Test X seconds for status change after ON/OFF
  ipmitool_path: Path to ipmitool binary
  shell_timeout: Wait X seconds for cmd prompt after issuing command
  port_as_ip: Make "port/plug" to be an alias to IP address
  retry_on: Count of attempts to retry power on
  sudo: Use sudo (without password) when calling 3rd party sotfware.
  priority: The priority of the stonith resource. Devices are tried in order of highest priority to lowest.
  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the cluster to use port 1 for node1 and ports 2 and 3 for node2
  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).
  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device), static-list (check the pcmk_host_list attribute), none (assume every device can
                   fence every machine)
  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using slow devices such as sbd. Use this to enable a random delay for
                  stonith actions. The overall delay is derived from this random delay value adding a static delay so that the sum is kept below the maximum delay.
  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays are configured on the nodes. Use this to enable a static delay for
                   stonith actions. The overall delay is derived from a random delay value adding this static delay so that the sum is kept below the maximum delay.
  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs to be configured first. Then use this to specify the maximum number
                     of actions can be performed in parallel on this device. -1 is unlimited.

Default operations:
  monitor: interval=60s
</code></pre>

<h3 id="toc_3">Libvirt Fencing</h3>

<p>If you are using KVM virtualisation we will use the fence_xvm fencing agent. This agent talks back to the hypervisor to power machines on/off. First let's check that we can see all of the available VM's</p>

<pre><code class="language-none">nodea # fence_xvm -o list
nodea.example.com              e3d38597-e90c-4bfb-b1d2-144c4ef615b5 on
nodeb.example.com              d3b46128-6df0-4e9d-a7c6-d5bc260a9920 on
nodec.example.com              802a74f3-a533-4e76-8135-267b14a193e7 on
</code></pre>

<p>We can now create the fencing resources in pacemaker.</p>

<pre><code class="language-none">nodea # pcs stonith create nodea-fence fence_xvm pcmk_host_map="nodea-priv.example.com:nodea.example.com"
nodea # pcs stonith create nodeb-fence fence_xvm pcmk_host_map="nodeb-priv.example.com:nodeb.example.com"
nodea # pcs stonith create nodec-fence fence_xvm pcmk_host_map="nodec-priv.example.com:nodec.example.com"
</code></pre>

<p>We can confirm the fencing resources are working by running pcs status or pcs stonith.</p>

<pre><code class="language-none">nodea # pcs stonith
 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
</code></pre>

<h3 id="toc_3">RHV-M Fencing</h3>

<p>For RHV VM's we can use the fence_rhvm agent. This talks to the RHV Manager API to power cycle the cluster nodes. We can check that we can talk to the RHV API and see our vm's.</p>

<pre><code class="language-none">nodea # fence_rhevm -a rhv-m.example.com -l 'admin@internal' -p 'Redhat123' -z -o list --ssl-insecure --login-timeout=30 --disable-http-filter | grep node
nodeb.example.com,
nodea.example.com,
nodec.example.com,
</code></pre>

<pre><code class="language-none">nodea # pcs stonith create nodea-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodea-priv.example.com:nodea.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
nodea # pcs stonith create nodeb-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodeb-priv.example.com:nodeb.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
nodea # pcs stonith create nodec-fence fence_rhevm ipaddr=rhvm.exmaple.com login='admin@internal' passwd='Redhat123' pcmk_host_map=nodec-priv.example.com:nodec.exmaple.com disable_http_filter=1 ssl_insecure=1 ssl=1
</code></pre>
</p>

<p>We can confirm the fencing resources are working by running pcs status or pcs stonith.</p>

<pre><code class="language-none">nodea # pcs stonith
 nodea-fence	(stonith:fence_rhevm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_rhevm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_rhevm):	Started nodeb-priv.example.com
</code></pre>

<h3 id="toc_3">Test Fencing</h3>

<p>Finally, let's test the fencing agent.</p>

<pre><code class="language-none">nodea # pcs stonith fence nodeb-priv.example.com</code></pre>

<p>Once the command prompt comes back we confirm the node has restarted. As these are virtual machines they restart quickly so we can follow the restart easily using watch.</p>

<pre><code class="language-none">nodea # watch pcs status 
Cluster name: cluster1
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 11:55:10 2018
Last change: Thu May 24 11:39:20 2018 by hacluster via crmd on nodeb.example.com

3 nodes configured
3 resources configured

Online: [ nodea-priv.example.com nodec-priv.example.com ]
OFFLINE: [ nodeb-priv.example.com ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodea-priv.example.com

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
</code></pre>

<p>The node should go OFFLINE before re-joining the cluster.</p>

<h2 id="toc_18">Prepare Cluster Resources</h2>

<p>We are going to configure a basic resource group which consists of the following.</p>

<ol>
<li>HA-LVM<br><br></li>
<li>Filesystem<br><br></li>
<li>Apache Web server<br><br></li>
<li>Virtual IP<br><br></li>
</ol>

<p>We need to prepare these resources before we can add them to the cluster.</p>

<h3 id="toc_18">iSCSI configuration</h2>

<p>NOTE: This is not required if we are deploying on RHV. 
First we need to discover the iSCSI targets and then login. We need to do this on <strong>all nodes</strong>.</p>

<pre><code class="language-none"># iscsiadm --mode discoverydb --type sendtargets --portal iscsi-storage.example.com --discover
# iscsiadm --mode node --targetname iqn.1994-05.com.redhat:iscsi-target --portal iscsi-storage.example.com --login
</code></pre>

<h3 id="toc_18">RHV Shared Storage</h2>

<p>For RHV environments we can create a shared virtual disk and add this to each node.</p>

<h3 id="toc_18">Confirm storage is visible</h2>

<p>dmesg should confirm we have discovered a disk - sda</p>


<pre><code class="language-none">[378055.438294] sd 2:0:0:0: [sda] 41934848 512-byte logical blocks: (21.4 GB/19.9 GiB)
[378055.438576] sd 2:0:0:0: [sda] Write Protect is off
[378055.438579] sd 2:0:0:0: [sda] Mode Sense: 43 00 10 08
[378055.438685] sd 2:0:0:0: [sda] Write cache: enabled, read cache: enabled, supports DPO and FUA
[378055.444792] sd 2:0:0:0: [sda] Attached SCSI disk</code></pre>

<h3 id="toc_18">LVM and Filesystem</h2>

<p>On <strong>nodea</strong> let's configure the LVM volume.</p>

<pre><code class="language-none">nodea # pvcreate /dev/sda
nodea # vgcreate ha_vg /dev/sda
nodea # lvcreate -L 5G -n ha_lv ha_vg
nodea # mkfs.xfs /dev/ha_vg/ha_lv
</code></pre>

<p>Now on <strong>each node</strong> we need to configure exclusive activation of a LVM volume group.</p>

<pre><code class="language-none"># lvmconf --enable-halvm --services --startstopservices</code></pre>

<p>Next we need ensure the local volume groups will still be activated outside of the cluster. Edit /etc/lvm/lvm.conf on <strong>each node </strong> and add the following line.</p>

<pre><code class="language-none">volume_list = [ "rhel" ]</code></pre>

<p>Rebuild the ramdisk on <strong>each node</strong> to ensure the nodes will only activate their local volume group and not the cluster volume groups.</p>

<pre><code class="language-none"># dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)
# reboot
</code></pre>

<h3 id="toc_18">Apache</h2>

<p>Now let's install apache on <strong>all nodes</strong> and enable http traffic through the firewall.</p>

<pre><code class="language-none"># yum -y install httpd php
# firewall-cmd --add-service=http --permanent
# firewall-cmd --add-service=http
</code></pre>

<h2 id="toc_18">Create and Manage Cluster Resources</h2>

<h3 id="toc_18">Creating Resources</h2>

<p>We can list all available resource types:</p>

<pre><code class="language-none"># pcs resource list</code></pre>

<p>To list the options for a particular resource:</p>

<pre><code class="language-none"># pcs resource describe LVM
Assumed agent name 'ocf:heartbeat:LVM' (deduced from 'LVM')
ocf:heartbeat:LVM - Controls the availability of an LVM Volume Group

Resource script for LVM. It manages an Linux Volume Manager volume (LVM) 
as an HA resource.

Resource options:
  volgrpname (required): The name of volume group.
  exclusive: If set, the volume group will be activated exclusively. This option works one of two ways. If the volume group has the cluster attribute
             set, then the volume group will be activated exclusively using clvmd across the cluster. If the cluster attribute is not set, the volume
             group will be activated exclusively through the use of the volume_list filter in lvm.conf. In the filter scenario, the LVM agent
             verifies that pacemaker's configuration will result in the volume group only being active on a single node in the cluster and that the
             local node's volume_list filter will prevent the volume group from activating outside of the resource agent. On activation this agent
             claims the volume group through the use of a unique tag, and then overrides the volume_list field in a way that allows the volume group
             to be activated only by the agent. To use exclusive activation without clvmd, the volume_list in lvm.conf must be initialized. If volume
             groups exist locally that are not controlled by the cluster, such as the root volume group, make sure those volume groups are listed in
             the volume_list so they will be allowed to activate on bootup.
  tag: If "exclusive" is set on a non clustered volume group, this overrides the tag to be used.
  partial_activation: If set, the volume group will be activated even only partial of the physical volumes available. It helps to set to true, when
                      you are using mirroring logical volumes.

Default operations:
  start: interval=0s timeout=30
  stop: interval=0s timeout=30
  monitor: interval=10 timeout=30
  methods: interval=0s timeout=5
</code></pre>

<p>Now we are going to create a resource group consisting of the following resources:</p>

<ol>
<li>HA-LVM<br><br></li>
<li>Filesystem<br><br></li>
<li>Apache Web server<br><br></li>
<li>Virtual IP<br><br></li>
</ol>
<p>Resources added to a resource group are started in the order they are added. First let's add the HA-LVM resource to activate the volume group.</p>

<pre><code class="language-none">nodea # pcs resource create www_vg LVM volgrpname=ha_vg exclusive=true --group webapp</code></pre>

<p>Next we need to add a filesystem. We will mount our filesystem at /var/www/html which is our document root.</p>

<pre><code class="language-none">nodea # pcs resource create www_filesystem filesystem device=/dev/ha_vg/ha_lv directory=/var/www/html fstype=xfs --group webapp</code></pre>

<p>The next resource we will add will be the virtual IP.</p>

<pre><code class="language-none">nodea # pcs resource create www_vip IPaddr2 ip=192.168.122.15 --group webapp</code></pre>

<p>The final resource we need is apache.</p>

<pre><code class="language-none">nodea # pcs resource create www_apache systemd:httpd --group webapp</code></pre>

<p>With the cluster resources online let's create a simple webpage in place which will show us the hostname we are running on. On the node running the resources create the file /var/www/html/index.php with the following contents.</p>

<pre><code class="language-none">&lt;!DOCTYPE html>
&lt;html>
&lt;body>

&lt;?php
echo gethostname();
?>

&lt;/body>
&lt;/html>
</code></pre>

<p>Set SELinux contexts.</p>

<pre><code class="language-none">nodea # cp /root/index.php /var/www/html/index.php
nodea # restorecon -Rv /var/www/html/
</code></pre>

<p>We should now be able to access our website on it's virtual IP address.</p>

<h3 id="toc_18">Managing Resources</h2>

<p>Let's quickly test if we can move our resource to a different node. First let's see what node it is running on.</p>

<pre><code class="language-none"># pcs status
Cluster name: cluster1
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Mon May 28 21:19:29 2018
Last change: Mon May 28 21:15:54 2018 by root via crm_resource on nodea.example.com

3 nodes configured
7 resources configured

Online: [ nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodea-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
 Resource Group: webapp
     ha_vg	(ocf::heartbeat:LVM):	Started nodea-priv.example.com
     www_filesystem	(ocf::heartbeat:Filesystem):	Started nodea-priv.example.com
     www_vip	(ocf::heartbeat:IPaddr2):	Started nodea-priv.example.com
     www_apache	(ocf::heartbeat:apache):	Started nodea-priv.example.com

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
</code></pre>


<p>Now let's move the resource group. We will come back to the message about location constraints.</p>

<pre><code class="language-none"># pcs resource move webapp
Warning: Creating location constraint cli-ban-webapp-on-nodea.example.com with a score of -INFINITY for resource webapp on node nodea.example.com.
This will prevent webapp from running on nodea.example.com until the constraint is removed. This will be the case even if nodea.example.com is the last node in the cluster.
</code></pre>

<p>Now check if the resources are running on a different node.</p>

<p>So what was the message about constraints? When we move a resource it creates a constraint which prevents the resource group from moving back to that node. This constraint can be viewed and cleared as follows.</p>

<pre><code class="language-none"># pcs constraint
Location Constraints:
  Resource: webapp
    Disabled on: nodea.example.com (score:-INFINITY) (role: Started)
Ordering Constraints:
Colocation Constraints:
Ticket Constraints:
</code></pre>

<p>To clear the constraint we can run the following.</p>
<pre><code class="language-none"># pcs resource clear webapp</code></pre>

<h4 id="toc_18">Stopping, starting & restarting Resources</h2>
<p>The following commands stop, start and restart a resource.</p>
<pre><code class="language-none"># pcs resource disable www_apache
# pcs resource enable www_apache
# pcs resource restart www_apache
</code></pre>

<h4 id="toc_18">Unmanaging resources</h4>
<p>Sometimes it can be useful to be able to stop and start resources outside of the clusters control. Unmanaging a resource stops pcs from actively manaing a resource. The below example would allow us to stop httpd without the cluster taking any action.</p>

<pre><code class="language-none"># pcs resource unmanage www_apache
# systemctl stop httpd
# pcs resource manage www_apache
</code></pre>

<h3 id="toc_18">Managing Nodes</h2>

<p>Sometimes we need to perform maintenance on nodes and need to either stop them from running resources or particular resources. Placing a node in standby stops it from running any resources.</p>

<pre><code class="language-none"># pcs cluster standby nodea-priv.example.com</code></pre>

<p>pcs status will confirm the node is now in standby. If it was running the cluster resources they will have moved to another node.</p>


<pre><code class="language-none"># pcs status
Cluster name: cluster1
Stack: corosync
Current DC: nodea.example.com (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 31 15:11:14 2018
Last change: Thu May 31 15:11:05 2018 by root via cibadmin on nodea.example.com

3 nodes configured
7 resources configured

Node nodea-priv.example.com: standby
Online: [ nodeb-priv.example.com nodec-priv.example.com ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
 nodeb-fence	(stonith:fence_xvm):	Started nodec-priv.example.com
 nodec-fence	(stonith:fence_xvm):	Started nodeb-priv.example.com
 Resource Group: webapp
     ha_vg	(ocf::heartbeat:LVM):	Started nodec-priv.example.com
     www_filesystem	(ocf::heartbeat:Filesystem):	Started nodec-priv.example.com
     www_vip	(ocf::heartbeat:IPaddr2):	Started nodec-priv.example.com
     www_app	(systemd:httpd):	Starting nodec-priv.example.com


Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
</code></pre>

<p>To enable the node to run resources again we need to unstandby it.</p>

<pre><code class="language-none"># pcs cluster unstandby nodea-priv.example.com</code></pre>

<p>Banning a resource just prevents that resource from running on that node. This may be useful if you have multiple resource groups and only want to prevent one of them from running on a node.</p>

<pre><code class="language-none"># pcs resource ban www_apache nodec-priv.example.com
# pcs resource clear www_apache
</code></pre>

<h3 id="toc_18">Node preference and resource stickiness</h2>

<p>A preferred node can be set for resources to ensure it always runs on that particular node. Let's make <strong>nodea</strong> our preferred node by giving it a score of 200. This is how much we want it to run on that node. On it's own that number doesn't mean a lot but we'll come back to that later.</p>

<pre><code class="language-none">nodea # pcs constraint location webapp prefers nodea-priv.example.com=200</code></pre>

<p>Now if we put <strong>nodea</strong> into standby the resource group will move to another cluster member. If we then unstandby it we should see the resource group move back to nodea.</p>

<pre><code class="language-none">nodea # pcs cluster standby nodea-priv.example.com
nodea # pcs cluster unstandby nodea-priv.example.com
</code></pre>

<p>The fact that the resource has moved back when it is in a perfectly healthy state may not be desireable. This causes unnecessary downtime for our application. This is where resource stickiness comes into play. This allows us to say how much we want our resource to stay where it currently is. If the score is higher than the score we gave to our preferred node then the resource should stay where it is.</p>

<p>Let's set our resource stickiness.</p>

<pre><code class="language-none">nodea # pcs resource defaults resource-stickiness=500</code></pre>

<p>Now let's repeat our test. The resource group should stay where it is.</p>

<pre><code class="language-none">nodea # pcs cluster standby nodea-priv.example.com
nodea # pcs cluster unstandby nodea-priv.example.com
</code></pre>

<h3 id="toc_18">Access pcsd Web UI</h2>

<p>The pcs Web UI can be accessed on any of the nodes. For example, browse to https://nodea.example.com:2224 and login with the hacluster credentials we setup when we installed the cluster.</p>

<p><center>
<img src="images/pcs_login.png"/>
</center></p>

<p>Once logged in, we can add our existing cluster. Select "Add Existing" and enter the name of one the nodes in the cluster. Then enter the password for the hacluster user if prompted.

We should now be able to manage the cluster through the web ui. An example of the resources screen can be seen.</p>

<p><center>
<img src="images/pcs_resources.png"/>
</center></p>

<h1 id="toc_17">Adding a node</h1>

<p>Now let's try adding nodes to the cluster. The workflow is very similar to the steps we took when creating a new cluster.</p>

<p>This time we'll start by installing apache on <strong>noded</strong> and allowing http traffic through the firewall.</p>

<pre><code class="language-none">noded # yum -y install httpd php
noded # firewall-cmd --add-service=http --permanent
noded # firewall-cmd --add-service=http
</code></pre>

<p>Now configure LVM.</p>

<pre><code class="language-none">noded # lvmconf --enable-halvm --services --startstopservices</code></pre>

<p>Edit /etc/lvm/lvm.conf and add the following line.</p>

<pre><code class="language-none">volume_list = [ "rhel" ]</code></pre>

<p>Rebuild the ramdisk and reboot.</p>

<pre><code class="language-none">noded # dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)
noded # reboot
</code></pre>

<p>On noded we need to add the storage. If this is a RHV environment then add the shared disk to the node. For an environment using iscsi then run the following commands.</p>

<pre><code class="language-none">noded # iscsiadm --mode discoverydb --type sendtargets --portal iscsi-storage.example.com --discover
noded # iscsiadm --mode node --targetname iqn.1994-05.com.redhat:iscsi-target --portal iscsi-storage.example.com --login
</code></pre>

<p>Now we can install the RHEL HA packages.</p>

<pre><code class="language-none">noded # yum install pcs pacemaker fence-agents-all</code></pre>

<p>Enable cluster communications through the firewall on noded.</p>

<pre><code class="language-none">noded # firewall-cmd --permanent --add-service=high-availability
noded # firewall-cmd --add-service=high-availability
</code></pre>

<p>Set password for the pcs administration account.</p>

<pre><code class="language-none">noded # echo Redhat123 | passwd --stdin hacluster</code></pre>

<p>Start and enable the pcs daemon on noded.</p>

<pre><code class="language-none">noded # systemctl enable pcsd && systemctl start pcsd </code></pre>

<p>On an existing node, authenticate the pcs admin user against the new node (noded in our example).</p>

<pre><code class="language-none">nodea # pcs cluster auth noded-priv.example.com</code></pre>

<p>On an existing node, add noded to the cluster.</p>

<pre><code class="language-none">nodea # pcs cluster node add noded-priv.example.com</code></pre>

<p>Finally, on noded we need to start and enable cluster services.</p>

<pre><code class="language-none">noded # pcs cluster start && pcs cluster enable</code></pre>

<p>Verify the new node has been succesfully added with pcs status.</p>

<p>Finally we can add the fencing device.</p>

<pre><code class="language-none">nodea # pcs stonith create noded-fence fence_xvm pcmk_host_map="noded-priv.example.com:noded.example.com"</code></pre>

<h3 id="toc_18">Activating LVM outside of the cluster</h2>

<p>It may be necessary to start the LVM volumes outside of the cluster control. Let's stop the resource group.</p>

<pre><code class="language-none">nodea # pcs resource disable webapp</code></pre>

<p>Now check that the volumegroup doesn't have any tags.</p>

<pre><code class="language-none">nodea # vgs -o tags
  VG Tags

</code></pre>

<p>Add a tag to the VG.</p>

<pre><code class="language-none">nodea # vgchange --addtag pacemaker ha_vg</code></pre>

<p>Now we can activate the volume group and mount the filesystem.</p>

<pre><code class="language-none">nodea # vgchange -ay --config activation{volume_list=[\"@pacemaker\"]} ha_vg
nodea # mount /dev/ha_vg/ha_lv /var/www/html
</code></pre>

<p>Once finished we can decactivate the volume group and restart the resource group.</p>

<pre><code class="language-none">nodea # umount /var/www/html
nodea # vgchange -an ha_vg
nodea # vgchange --deltag pacemaker ha_vg
nodea # pcs resource enable webapp
</code></pre>

<h2 id="toc_2">Testing</h2>

<h3 id="toc_3">Offline and Online Resources</h3>

<p>Attempt to offline and online a resource using either the pcs gui or via the cli. </p>

<pre><code class="language-none">nodea # pcs resource disable www_app
nodea # pcs resource enable www_app
</code></pre>

<h3 id="toc_3">Move resource group</h3>

<p>Migrate the resource group to another node.</p>

<pre><code class="language-none">nodea # pcs resource move webapp</code></pre>

<h3 id="toc_3">Shutdown and start cluster</h3>

<p>Stop cluster services on all nodes.</p>

<pre><code class="language-none">nodea # pcs cluster stop --all</code></pre>

<p>Stop cluster services on all nodes.</p>

<pre><code class="language-none">nodea # pcs cluster start --all</code></pre>

<h3 id="toc_3">Stop resources outside of cluster</h3>

<p>Place a resource in standby and then stop it outside of the cluster control.</p>

<pre><code class="language-none">nodea # pcs resource unmanage www_app
nodea # systemctl stop httpd
</code></pre>

<p>Unstandby the resource when finished.</p>
<pre><code class="language-none">nodea # pcs resource manage www_app</code></pre>

<h3 id="toc_3">Resource group failure testing</h3>

<p>Try stopping the httpd service with - systemctl stop httpd - You'll notice that the resource is restarted on the same node. This is the default behaviour with RHEL HA. We can tell the resource to failover to another node after X number of failures. Let's take a look at the failcount for the resource.</p>

<pre><code class="language-none">nodea # crm_failcount -G -r www_apache
scope=status  name=fail-count-www_apache value=1
</code></pre>

<p>First we can clear the failcount and then let's configure the cluster to move a failed resource after 1 failure.</p>

<pre><code class="language-none">nodea # crm_failcount -D -r www_apache
nodea # pcs resource defaults migration-threshold=1
</code></pre>

<p>Now if we stop httpd we should see the resource move to a different node. Clear the failcount when finished.</p>

<pre><code class="language-none">nodea # crm_failcount -D -r www_apache</code></pre>

<h3 id="toc_3">Crash cluster node</h3>

<p>Identify the active node in the cluster and cause it to crash.</p>

<pre><code class="language-none"># echo 1 > /proc/sys/kernel/sysrq
# echo c > /proc/sysrq-trigger
</code></pre>

<h3 id="toc_3">Making resources non-critical</h3>

<p>A resource can be marked as non critical to prevent a failover of the resource group.</p>

<pre><code class="language-none">nodea # pcs resource update www_apache op monitor on-fail=ignore</code></pre>

<p>Now if we stop apache we should see a message saying that the failure has been ignored.</p>

<pre><code class="language-none"># systemctl stop httpd</code></pre>

<h2 id="toc_2">GFS clustered filesystem</h2>

<p>We are going to create a 3 node cluster with gfs2 installed. We'll run apache on each node providing an active/active configuration. We'll only configure nodea, nodeb and nodec for this example.</p>

<h3 id="toc_3">Create the cluster</h3>

<p>On <strong>each node</strong> enable the resilient storage repository and install the gfs2 packages.</p>

<pre><code class="language-none"># subscription-manager repos --enable=rhel-rs-for-rhel-7-server-rpms
# yum install lvm2-cluster gfs2-utils -y</code></pre>

<p>Now setup the cluster again. Start with the install of packages and start pcsd.</p>

<pre><code class="language-none"># yum install pcs pacemaker fence-agents-all -y
# firewall-cmd --permanent --add-service=high-availability
# firewall-cmd --add-service=high-availability
# echo Redhat123 | passwd --stdin hacluster
# systemctl enable pcsd && systemctl start pcsd
</code></pre>

<p>Next, configure iSCSI storage and install httpd on <strong>all nodes</strong>.</p>

<pre><code class="language-none"># iscsiadm --mode discoverydb --type sendtargets --portal iscsi-storage.example.com --discover
# iscsiadm --mode node --targetname iqn.1994-05.com.redhat:iscsi-target --portal iscsi-storage.example.com --login

# firewall-cmd --add-service=http
# yum -y install httpd
</code></pre>

<p>Now on <strong>nodea</strong> we can create the cluster as before.</p>

<pre><code class="language-none">nodea # pcs cluster auth nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com
nodea # pcs cluster setup --start --enable --name cluster1 nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com

nodea # pcs stonith create nodea-fence fence_xvm pcmk_host_map="nodea-priv.example.com:nodea.example.com"
nodea # pcs stonith create nodeb-fence fence_xvm pcmk_host_map="nodeb-priv.example.com:nodeb.example.com"
nodea # pcs stonith create nodec-fence fence_xvm pcmk_host_map="nodec-priv.example.com:nodec.example.com"
</code></pre>

<h3 id="toc_3">Configure GFS2</h3>

<p>Now that we have a basic cluster we can start to configure GFS2 on <strong>nodea</strong>. We need to change the way the cluster behaves when we lose quorum. We also need to create a resource for the dynamic locking manager (dlm). dlm is the mechanism which allows each node to maintain it's own cache. Note the use of the clone option. This creates a running copy of the resource on each node.</p>

<pre><code class="language-none">nodea # pcs property set no-quorum-policy=freeze
nodea # pcs resource create dlm ocf:pacemaker:controld op monitor interval=30s on-fail=fence clone interleave=true ordered=true
</code></pre>

<p>Now on <strong>all nodes</strong> we need to configure LVM to use cluster-wide locking.</p>

<pre><code class="language-none"># /sbin/lvmconf --enable-cluster</code></pre>

<p>Now back on <strong>nodea</strong> we can create the clustered logical volume resource (clvmd). We will also create some resource constraints. Firstly, to ensure the dlm and clvmd resources are colocated on each node. Secondly, to ensure dlm starts before we try to start clvmd.</p>

<pre><code class="language-none">nodea # pcs resource create clvmd ocf:heartbeat:clvm op monitor interval=30s on-fail=fence clone interleave=true ordered=true
nodea # pcs constraint order start dlm-clone then clvmd-clone
nodea # pcs constraint colocation add clvmd-clone with dlm-clone</code></pre>

<p>We can now configure LVM. Note the -c option used with vgcreate. This creates a clustered volume group.<p>

<pre><code class="language-none">nodea # pvcreate /dev/sda
nodea # vgcreate -Ay -cy cluster_vg /dev/sda
nodea # lvcreate -L5G -n cluster_lv cluster_vg
</code></pre>

<p>Next we can create the gfs2 filesystem. Each node needs a journal which we specify with -j3. The -t options specifies the lock table name. The first part of the name must match the pacemaker cluster name. Only members of this cluster can use the gfs2 filesystem.</p>

<pre><code class="language-none">nodea # mkfs.gfs2 -j3 -p lock_dlm -t cluster1:www_gfs2 /dev/cluster_vg/cluster_lv</code></pre>

<p>Now we can add the gfs2 filesystem as a resource to our cluster. Again, we need to create some resource constraints to configure ordering and co-location.</p>

<pre><code class="language-none">nodea # pcs resource create www_gfs2 Filesystem device="/dev/cluster_vg/cluster_lv" directory="/var/www/html" fstype="gfs2" "options=noatime" op monitor interval=10s on-fail=fence clone interleave=true
nodea # pcs constraint order start clvmd-clone then www_gfs2-clone
nodea # pcs constraint colocation add www_gfs2-clone with clvmd-clone
</code></pre>

<p>We can create an apache resource to serve some content from our shared filesystem. Note we are using the clone option. Once again, we are also creating constraints.</p>

<pre><code class="language-none">nodea # pcs resource create apache systemd:httpd clone interleave=true ordered=true
nodea # pcs constraint order start www_gfs2-clone then apache-clone
nodea # pcs constraint colocation add apache-clone with www_gfs2-clone</code></pre>

<p>Now on <strong>nodea</strong> let's echo something into /var/www/html/index.html so that we can see all 3 nodes serving the same content.</p>

<pre><code class="language-none">nodea # echo GFS2 > /var/www/html/index.html
nodea # restorecon -R /var/www/html
</code></pre>
